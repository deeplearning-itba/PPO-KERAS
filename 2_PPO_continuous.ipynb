{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from REINFORCE_helper import RunningVariance\n",
    "from time import time\n",
    "from REINFORCE_helper import BaseAgent\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_CLIPPING = 0.2 # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "\n",
    "NOISE = 2.0\n",
    "\n",
    "class ReinforceAgent(BaseAgent):\n",
    "    def proximal_policy_optimization_loss_continuous(self, advantage, old_prediction):\n",
    "        def loss(y_true, y_pred):\n",
    "            var = K.square(self.noise)\n",
    "            denom = K.sqrt(2 * np.pi * var)\n",
    "            prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n",
    "            old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))\n",
    "\n",
    "            prob = prob_num/denom\n",
    "            old_prob = old_prob_num/denom\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "\n",
    "            return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage))\n",
    "        return loss\n",
    "\n",
    "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
    "        def actor_loss(y_true, y_pred):\n",
    "            var = K.square(self.noise)\n",
    "            denom = K.sqrt(2 * np.pi * var)\n",
    "            prob_num = K.exp(- K.square(y_true - y_pred) / (2 * var))\n",
    "            old_prob_num = K.exp(- K.square(y_true - old_prediction) / (2 * var))\n",
    "\n",
    "            prob = prob_num/denom\n",
    "            old_prob = old_prob_num/denom\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "            return K.mean(r)\n",
    "        \n",
    "        state_input = Input(shape=input_shape)\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(output_shape,))\n",
    "\n",
    "        x = Dense(hidden_layer_neurons, activation='relu')(state_input)\n",
    "#         x = Dense(256, activation='selu')(x)\n",
    "#         x = Dense(256, activation='selu')(x)\n",
    "        \n",
    "        out_actions = Dense(output_shape, activation='tanh', name='output')(x)\n",
    "\n",
    "        model_train = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model_predict = Model(inputs=[state_input], outputs=[out_actions])\n",
    "        \n",
    "        model_train.compile(Adam(lr), loss=[self.proximal_policy_optimization_loss_continuous(advantage, old_prediction)], metrics=[actor_loss])\n",
    "        return model_train, model_predict\n",
    "    \n",
    "    def get_action(self, eval=False):\n",
    "        obs = self.scaler.transform(self.observation.reshape(1, self.nS))\n",
    "        obs = self.observation.reshape(1, self.nS)\n",
    "        p = self.model_predict.predict(obs)*self.env.action_space.high # Esto hay que corregirlo si no es simetrico\n",
    "        if eval is False:\n",
    "            action = action_one_hot = p[0] + np.random.normal(loc=0, scale=self.noise, size=p[0].shape)\n",
    "        else:\n",
    "            action = action_one_hot = p[0]\n",
    "        \n",
    "        return action, action_one_hot, p\n",
    "    \n",
    "    \n",
    "    def get_critic_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=1):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
    "#         model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='selu'))\n",
    "        model.add(Dense(output_shape, activation='linear'))\n",
    "        model.compile(Adam(lr), loss=['mse'])\n",
    "        return model\n",
    "    \n",
    "    def get_discounted_rewards(self, r):\n",
    "        # Por si es una lista\n",
    "        r = np.array(r, dtype=float)\n",
    "        \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: El espacio de acción es continuo\n",
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('MountainCarContinuous-v0', n_experience_episodes=10, EPISODES=2000, epochs=1, \n",
    "                                 lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 513\n",
      "Trainable params: 513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.model_predict.summary()\n",
    "reinforce_agent.model_predict.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 513\n",
      "Trainable params: 513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 2) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 1) dtype=float32>,\n",
       " <tf.Tensor 'input_3:0' shape=(?, 1) dtype=float32>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.model_train.summary()\n",
    "reinforce_agent.model_train.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(values, rewards, gamma=0.999, lmbda=0.95):\n",
    "    #GAE\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] - values[i]\n",
    "        gae = delta + gamma * lmbda * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_targets(rewards, values, gamma=0.999, n_steps = 5):\n",
    "    out = []\n",
    "    ep_len = len(rewards)\n",
    "    gammas = np.power(gamma, range(ep_len+1)) # El +1 es un hack para que no cuelgue cuando le pasamos n_steps = len(rewards)\n",
    "    padded_values = np.vstack([values, np.zeros([n_steps, 1])])\n",
    "    for t in range(ep_len):\n",
    "        # t desde donde comienzo, por ejemplo si t=0 sumo desde 0 a n_steps-1\n",
    "        rewards_left = min([0, ep_len-t-n_steps])\n",
    "        first_term = (gammas[:(n_steps+rewards_left)]*rewards[t:t+n_steps]).sum()\n",
    "        A_t =  first_term - padded_values[t] + gammas[n_steps]*padded_values[t+n_steps]\n",
    "        out.append(A_t)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AC_Advantages(rewards, gamma, values):\n",
    "    return rewards.reshape(-1,1) + gamma*values[1:] - values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: El espacio de acción es continuo\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('Pendulum-v0', n_experience_episodes=3, EPISODES=2000, epochs=1, \n",
    "                                 lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32, noise=1.0)\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.], dtype=float32), array([-2.], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.env.action_space.high, reinforce_agent.env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.7000082672385384, -2.9571554798133577)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(actions), np.min(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=3, EPISODES=2000, epochs=1, \n",
    "#                                  lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)\n",
    "# obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(get_advantages(values_, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: El espacio de acción es continuo\n",
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 13\n",
      "Model on episode 14 improved from -inf to -0.005498244665187006. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|▏         | 14/1000 [00:00<00:07, 131.86it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpszmesi8b.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 27\n",
      "Model on episode 28 improved from -0.005498244665187006 to -0.004996542638688931. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|▏         | 14/1000 [00:00<00:07, 130.92it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmp7yx8go_y.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 41\n",
      "Model on episode 42 improved from -0.004996542638688931 to -0.0009536423694723672. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|▏         | 14/1000 [00:00<00:07, 131.15it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpjwf6q30v.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 55\n",
      "Model on episode 56 did not improved -0.008955267669767213. Best saved: -0.0009536423694723672\n",
      "Episode: 69\n",
      "Model on episode 70 did not improved -0.009671500552919297. Best saved: -0.0009536423694723672\n",
      "Episode: 83\n",
      "Model on episode 84 did not improved -0.002833434860154818. Best saved: -0.0009536423694723672\n",
      "Episode: 97\n",
      "Model on episode 98 did not improved -0.0077460746851736985. Best saved: -0.0009536423694723672\n",
      "Episode: 111\n",
      "Model on episode 112 did not improved -0.007675717791928941. Best saved: -0.0009536423694723672\n",
      "Episode: 125\n",
      "Model on episode 126 did not improved -0.06893483614877022. Best saved: -0.0009536423694723672\n",
      "Episode: 139\n",
      "Model on episode 140 did not improved -0.027616102505510148. Best saved: -0.0009536423694723672\n",
      "Episode: 153\n",
      "Model on episode 154 did not improved -0.06188588051432109. Best saved: -0.0009536423694723672\n",
      "Episode: 167\n",
      "Model on episode 168 improved from -0.0009536423694723672 to 4.406501848450448. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 14/308 [00:00<00:02, 130.97it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmprjleefmo.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 181\n",
      "Model on episode 182 improved from 4.406501848450448 to 7.180220412191438. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 12/256 [00:00<00:02, 118.33it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpvcydcdk_.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 195\n",
      "Model on episode 196 improved from 7.180220412191438 to 19.93930902287264. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   9%|▉         | 14/151 [00:00<00:01, 130.37it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpier9xnoi.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/85 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model on episode 210 improved from 19.93930902287264 to 41.334334464702515. Saved!\n",
      "MoviePy - Building file /tmp/tmp2bxhqev2.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 223\n",
      "Model on episode 224 did not improved 14.118364358540443. Best saved: 41.334334464702515\n",
      "Episode: 237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|          | 0/78 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model on episode 238 improved from 41.334334464702515 to 43.44927389199169. Saved!\n",
      "MoviePy - Building file /tmp/tmpxqrxgy27.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 251\n",
      "Model on episode 252 did not improved 38.43489637588831. Best saved: 43.44927389199169\n",
      "Episode: 265\n",
      "Model on episode 266 did not improved 25.32755289796818. Best saved: 43.44927389199169\n",
      "Episode: 279\n",
      "Model on episode 280 did not improved 28.994050205931856. Best saved: 43.44927389199169\n",
      "Episode: 293\n",
      "Model on episode 294 did not improved 28.527438278674193. Best saved: 43.44927389199169\n",
      "Episode: 307\n",
      "Model on episode 308 did not improved 29.271368625626206. Best saved: 43.44927389199169\n",
      "Episode: 321\n",
      "Model on episode 322 did not improved 40.781048181102996. Best saved: 43.44927389199169\n",
      "Episode: 335\n",
      "Model on episode 336 did not improved 38.73822708812725. Best saved: 43.44927389199169\n",
      "Episode: 349\n",
      "Model on episode 350 did not improved 39.95342217919006. Best saved: 43.44927389199169\n",
      "Episode: 363\n",
      "Model on episode 364 did not improved 26.893785018199146. Best saved: 43.44927389199169\n",
      "Episode: 377\n",
      "Model on episode 378 did not improved 27.93257457419002. Best saved: 43.44927389199169\n",
      "Episode: 391\n",
      "Model on episode 392 did not improved 27.978727544589237. Best saved: 43.44927389199169\n",
      "Episode: 405\n",
      "Model on episode 406 did not improved 28.03873636735733. Best saved: 43.44927389199169\n",
      "Episode: 419\n",
      "Model on episode 420 did not improved 28.217269214745095. Best saved: 43.44927389199169\n",
      "Episode: 433\n",
      "Model on episode 434 did not improved 26.014838689107307. Best saved: 43.44927389199169\n",
      "Episode: 447\n",
      "Model on episode 448 did not improved 27.382522701732327. Best saved: 43.44927389199169\n",
      "Episode: 461\n",
      "Model on episode 462 did not improved 39.69996427686369. Best saved: 43.44927389199169\n",
      "Episode: 475\n",
      "Model on episode 476 did not improved 41.0983003004377. Best saved: 43.44927389199169\n",
      "Episode: 489\n",
      "Model on episode 490 did not improved 40.980906021149046. Best saved: 43.44927389199169\n",
      "Episode: 503\n",
      "Model on episode 504 did not improved 38.57513164690666. Best saved: 43.44927389199169\n"
     ]
    }
   ],
   "source": [
    "critic_lr = 0.01\n",
    "actor_lr =  0.01\n",
    "LOSS_CLIPPING = 0.2 # Recomendado por el Paper\n",
    "\n",
    "reinforce_agent = ReinforceAgent('MountainCarContinuous-v0', n_experience_episodes=4, EPISODES=500, epochs=10, eval_period=12,\n",
    "                                 LOSS_CLIPPING=LOSS_CLIPPING,\n",
    "                                 lr=actor_lr, algorithm='PPO', \n",
    "                                 gif_to_board=True, batch_size=64, gamma=0.99, noise=1.0)\n",
    "\n",
    "\n",
    "initial_time = time()\n",
    "running_variance = RunningVariance()\n",
    "critic_model = reinforce_agent.get_critic_model(lr=critic_lr, \n",
    "                                           hidden_layer_neurons=128,\n",
    "                                           input_shape=[reinforce_agent.nS],\n",
    "                                           output_shape=1)\n",
    "\n",
    "###########################################\n",
    "## Entreno V(s) para que no tenga basura ##\n",
    "###########################################\n",
    "# Corro episodios con policy random\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "\n",
    "# Les saco la ultima observación por que no tiene reward\n",
    "observations = []\n",
    "for i in range(reinforce_agent.n_experience_episodes):\n",
    "    observations.append(obs[i][:-1])\n",
    "observations = np.vstack(observations)\n",
    "\n",
    "# Entreno V(s)\n",
    "history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "\n",
    "\n",
    "###########################################\n",
    "## Ciclo de entrenamiento del modelo     ##\n",
    "###########################################\n",
    "\n",
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    # Corro episodio con policy que se irá entrenando\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "    actions = np.vstack(actions) # Pongo todas las acciones de los distintos episodios juntas\n",
    "    # Pongo las predicciones juntas y las guardo como las viejas para pasarselas al modelo\n",
    "    # Las nuevas predicciones será la salida de la red neuronal\n",
    "    old_prediction = np.vstack(preds) \n",
    "    \n",
    "    # Calculo advantages y guardo observaciones sin la última observación\n",
    "    advantage = []\n",
    "    observations = []\n",
    "    for i in range(reinforce_agent.n_experience_episodes):\n",
    "        values = critic_model.predict(obs[i]) \n",
    "#         values_ = np.vstack([rewards[i].reshape(-1,1) + reinforce_agent.gamma*values[1:], 0])\n",
    "        \n",
    "        advantage.append(get_advantages(values, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))\n",
    "#         advantage.append(get_AC_Advantages(rewards[i], reinforce_agent.gamma, values))\n",
    "        observations.append(obs[i][:-1])\n",
    "        \n",
    "    advantage = np.vstack(advantage)\n",
    "    observations = np.vstack(observations)\n",
    "    \n",
    "    # Calculo de varianza\n",
    "    for ad in advantage:\n",
    "        running_variance.add(ad)\n",
    "\n",
    "    # Normalización de advantage\n",
    "    advantage = (advantage-advantage.mean()) / advantage.std()\n",
    "    \n",
    "    # Entrenamiento de Policy\n",
    "    history_loss = reinforce_agent.model_train.fit([observations, advantage, old_prediction], \n",
    "                                                   actions, verbose=0, \n",
    "                                                   epochs=reinforce_agent.epochs, \n",
    "                                                   batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "#     disc_sum_rews = (disc_sum_rews - disc_sum_rews.mean()) / disc_sum_rews.std()\n",
    "    # Entrenamiento de V(s)\n",
    "    history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "    # Logue de resultados\n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history_loss.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      None, \n",
    "                      running_variance.get_variance(), \n",
    "                      history_loss.history['actor_loss'][0], \n",
    "                      time() - initial_time, np.mean(ep_returns[-1]), \n",
    "                      history_critic.history['loss'][0])\n",
    "    \n",
    "reinforce_agent.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: El espacio de acción es continuo\n",
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 51\n",
      "Model on episode 52 improved from -inf to -465.68147973825245. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 92.99it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpsr3p86sn.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 103\n",
      "Model on episode 104 improved from -465.68147973825245 to -436.14260197375035. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 95.40it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpiglalb2p.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 155\n",
      "Model on episode 156 improved from -436.14260197375035 to -428.0049757427229. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 94.46it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmppvyguixd.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 207\n",
      "Model on episode 208 did not improved -679.4313999738971. Best saved: -428.0049757427229\n",
      "Episode: 259\n",
      "Model on episode 260 did not improved -556.0535486669834. Best saved: -428.0049757427229\n",
      "Episode: 311\n",
      "Model on episode 312 did not improved -631.9602354049055. Best saved: -428.0049757427229\n",
      "Episode: 363\n",
      "Model on episode 364 improved from -428.0049757427229 to -360.83892673867075. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 93.77it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpz7u7nuqx.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 415\n",
      "Model on episode 416 did not improved -611.6465408463341. Best saved: -360.83892673867075\n",
      "Episode: 467\n",
      "Model on episode 468 did not improved -642.907656693919. Best saved: -360.83892673867075\n",
      "Episode: 519\n",
      "Model on episode 520 did not improved -687.8188154098955. Best saved: -360.83892673867075\n",
      "Episode: 571\n",
      "Model on episode 572 did not improved -558.5294202997267. Best saved: -360.83892673867075\n",
      "Episode: 623\n",
      "Model on episode 624 did not improved -645.8386610442902. Best saved: -360.83892673867075\n",
      "Episode: 675\n",
      "Model on episode 676 did not improved -648.7655222725291. Best saved: -360.83892673867075\n",
      "Episode: 727\n",
      "Model on episode 728 did not improved -650.7627972459811. Best saved: -360.83892673867075\n",
      "Episode: 779\n",
      "Model on episode 780 did not improved -587.1545632857967. Best saved: -360.83892673867075\n",
      "Episode: 831\n",
      "Model on episode 832 did not improved -469.3759785536423. Best saved: -360.83892673867075\n",
      "Episode: 883\n",
      "Model on episode 884 did not improved -568.2531774999143. Best saved: -360.83892673867075\n",
      "Episode: 935\n",
      "Model on episode 936 did not improved -682.7968024278679. Best saved: -360.83892673867075\n",
      "Episode: 987\n",
      "Model on episode 988 did not improved -642.5340608003069. Best saved: -360.83892673867075\n",
      "Episode: 1039\n",
      "Model on episode 1040 did not improved -644.3426426485698. Best saved: -360.83892673867075\n",
      "Episode: 1091\n",
      "Model on episode 1092 did not improved -648.1460495224858. Best saved: -360.83892673867075\n",
      "Episode: 1143\n",
      "Model on episode 1144 did not improved -596.2912539457881. Best saved: -360.83892673867075\n",
      "Episode: 1195\n",
      "Model on episode 1196 did not improved -518.0216797548146. Best saved: -360.83892673867075\n",
      "Episode: 1247\n",
      "Model on episode 1248 did not improved -541.3441965099254. Best saved: -360.83892673867075\n",
      "Episode: 1299\n",
      "Model on episode 1300 did not improved -643.6214248783488. Best saved: -360.83892673867075\n",
      "Episode: 1351\n",
      "Model on episode 1352 did not improved -648.162259700006. Best saved: -360.83892673867075\n",
      "Episode: 1403\n",
      "Model on episode 1404 did not improved -463.966403590104. Best saved: -360.83892673867075\n",
      "Episode: 1455\n",
      "Model on episode 1456 improved from -360.83892673867075 to -121.73531820733947. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 91.94it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmp0bflxp_y.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1507\n",
      "Model on episode 1508 did not improved -645.6825779901825. Best saved: -121.73531820733947\n",
      "Episode: 1559\n",
      "Model on episode 1560 improved from -121.73531820733947 to -3.2132599800796178. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:01, 98.66it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmplu7vrc3m.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1611\n",
      "Model on episode 1612 did not improved -541.1204099697152. Best saved: -3.2132599800796178\n",
      "Episode: 1663\n",
      "Model on episode 1664 did not improved -601.233195907849. Best saved: -3.2132599800796178\n",
      "Episode: 1715\n",
      "Model on episode 1716 did not improved -654.7230864777673. Best saved: -3.2132599800796178\n",
      "Episode: 1767\n",
      "Model on episode 1768 did not improved -650.370481032233. Best saved: -3.2132599800796178\n",
      "Episode: 1819\n",
      "Model on episode 1820 did not improved -645.8343341543684. Best saved: -3.2132599800796178\n",
      "Episode: 1871\n",
      "Model on episode 1872 did not improved -641.3875895614188. Best saved: -3.2132599800796178\n",
      "Episode: 1923\n",
      "Model on episode 1924 did not improved -539.9637729316637. Best saved: -3.2132599800796178\n",
      "Episode: 1975\n",
      "Model on episode 1976 did not improved -595.8623433649253. Best saved: -3.2132599800796178\n",
      "Episode: 2027\n",
      "Model on episode 2028 did not improved -664.2545147730386. Best saved: -3.2132599800796178\n",
      "Episode: 2079\n",
      "Model on episode 2080 did not improved -665.9794248184882. Best saved: -3.2132599800796178\n",
      "Episode: 2131\n",
      "Model on episode 2132 improved from -3.2132599800796178 to -1.5910330501839158. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 95.96it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmptgks4wbe.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2183\n",
      "Model on episode 2184 did not improved -626.5182051258429. Best saved: -1.5910330501839158\n",
      "Episode: 2235\n",
      "Model on episode 2236 did not improved -639.8613335180277. Best saved: -1.5910330501839158\n",
      "Episode: 2287\n",
      "Model on episode 2288 did not improved -705.3190147891942. Best saved: -1.5910330501839158\n",
      "Episode: 2339\n",
      "Model on episode 2340 did not improved -637.7986410299651. Best saved: -1.5910330501839158\n",
      "Episode: 2391\n",
      "Model on episode 2392 did not improved -2.545848427187707. Best saved: -1.5910330501839158\n",
      "Episode: 2443\n",
      "Model on episode 2444 improved from -1.5910330501839158 to -1.1701681561977368. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   5%|▍         | 10/202 [00:00<00:02, 91.56it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmp50nkoos3.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2495\n",
      "Model on episode 2496 did not improved -576.2141755061385. Best saved: -1.1701681561977368\n",
      "Episode: 2547\n",
      "Model on episode 2548 did not improved -632.1587018600778. Best saved: -1.1701681561977368\n",
      "Episode: 2599\n",
      "Model on episode 2600 did not improved -632.622655718945. Best saved: -1.1701681561977368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dc52a32e84cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m                                                    \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreinforce_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                                                    batch_size=reinforce_agent.batch_size)\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m#     disc_sum_rews = (disc_sum_rews - disc_sum_rews.mean()) / disc_sum_rews.std()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    192\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[1;32m    193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "critic_lr = 0.001\n",
    "actor_lr =  0.001\n",
    "LOSS_CLIPPING = 0.2 # Recomendado por el Paper 0.2\n",
    "\n",
    "reinforce_agent = ReinforceAgent('Pendulum-v0', n_experience_episodes=10, EPISODES=4000, epochs=10, eval_period=50,\n",
    "                                 lr=actor_lr, algorithm='PPO', gif_to_board=True, batch_size=64, gamma=0.99, noise=0.5, \n",
    "                                 LOSS_CLIPPING=LOSS_CLIPPING)\n",
    "\n",
    "# reinforce_agent = ReinforceAgent('CartPole-v0', n_experience_episodes=1, EPISODES=2000, epochs=1, \n",
    "#                                  lr=actor_lr, algorithm='PPO', gif_to_board=False, batch_size=64)\n",
    "\n",
    "initial_time = time()\n",
    "running_variance = RunningVariance()\n",
    "critic_model = reinforce_agent.get_critic_model(lr=critic_lr, \n",
    "                                           hidden_layer_neurons=128,\n",
    "                                           input_shape=[reinforce_agent.nS],\n",
    "                                           output_shape=1)\n",
    "\n",
    "###########################################\n",
    "## Entreno V(s) para que no tenga basura ##\n",
    "###########################################\n",
    "# Corro episodios con policy random\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "\n",
    "# Les saco la ultima observación por que no tiene reward\n",
    "observations = []\n",
    "for i in range(reinforce_agent.n_experience_episodes):\n",
    "    observations.append(obs[i][:-1])\n",
    "observations = np.vstack(observations)\n",
    "\n",
    "# Entreno V(s)\n",
    "history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "\n",
    "\n",
    "###########################################\n",
    "## Ciclo de entrenamiento del modelo     ##\n",
    "###########################################\n",
    "\n",
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    # Corro episodio con policy que se irá entrenando\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "    actions = np.vstack(actions) # Pongo todas las acciones de los distintos episodios juntas\n",
    "    # Pongo las predicciones juntas y las guardo como las viejas para pasarselas al modelo\n",
    "    # Las nuevas predicciones será la salida de la red neuronal\n",
    "    old_prediction = np.vstack(preds) \n",
    "    \n",
    "    # Calculo advantages y guardo observaciones sin la última observación\n",
    "    advantage = []\n",
    "    observations = []\n",
    "    for i in range(reinforce_agent.n_experience_episodes):\n",
    "        values = critic_model.predict(obs[i]) \n",
    "#         values_ = np.vstack([rewards[i].reshape(-1,1) + reinforce_agent.gamma*values[1:], 0])\n",
    "        \n",
    "        advantage.append(get_advantages(values, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))\n",
    "#         advantage.append(get_AC_Advantages(rewards[i], reinforce_agent.gamma, values))\n",
    "        observations.append(obs[i][:-1])\n",
    "        \n",
    "    advantage = np.vstack(advantage)\n",
    "    observations = np.vstack(observations)\n",
    "    \n",
    "    # Calculo de varianza\n",
    "    for ad in advantage:\n",
    "        running_variance.add(ad)\n",
    "\n",
    "    # Normalización de advantage\n",
    "    advantage = (advantage-advantage.mean()) / advantage.std()\n",
    "    \n",
    "    # Entrenamiento de Policy\n",
    "    history_loss = reinforce_agent.model_train.fit([observations, advantage, old_prediction], \n",
    "                                                   actions, verbose=0, \n",
    "                                                   epochs=reinforce_agent.epochs, \n",
    "                                                   batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "#     disc_sum_rews = (disc_sum_rews - disc_sum_rews.mean()) / disc_sum_rews.std()\n",
    "    # Entrenamiento de V(s)\n",
    "    history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "    # Logue de resultados\n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history_loss.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      None, \n",
    "                      running_variance.get_variance(), \n",
    "                      None, \n",
    "                      time() - initial_time, np.mean(ep_returns[-1]), \n",
    "                      history_critic.history['loss'][0])\n",
    "    \n",
    "reinforce_agent.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
