{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentación PPO\n",
    "\n",
    "https://docs.google.com/presentation/d/1Pd9z3rWLPlNmV-AbMBxhpvGMWCWAQEWDfeFlJZy40WY/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from REINFORCE_helper import RunningVariance\n",
    "from time import time\n",
    "from REINFORCE_helper import BaseAgent\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [1.5 0.5 1.  0.9 1.1 5. ]\n",
      "clipped : [1.2 0.8 1.  0.9 1.1 1.2]\n",
      "Minimim : [1.2 0.5 1.  0.9 1.1 1.2]\n"
     ]
    }
   ],
   "source": [
    "LOSS_CLIPPING = 0.2\n",
    "r = np.array([1.5, 0.5, 1, 0.9, 1.1, 5 ])\n",
    "adv = 1.0\n",
    "print('original:', r*adv) \n",
    "print('clipped :', adv*np.clip(r, 1 - LOSS_CLIPPING, 1 + LOSS_CLIPPING))\n",
    "print('Minimim :', np.minimum(r*adv, adv*np.clip(r, 1 - LOSS_CLIPPING, 1 + LOSS_CLIPPING)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent(BaseAgent):\n",
    "    def proximal_policy_optimization_loss(self, advantage, old_prediction):\n",
    "        def loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred\n",
    "            old_prob = y_true * old_prediction\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "            return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - self.LOSS_CLIPPING, max_value=1 + self.LOSS_CLIPPING) * advantage) \n",
    "                           + self.ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "        return loss\n",
    "\n",
    "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
    "        ## Defino métrica - loss sin el retorno multiplicando\n",
    "        def actor_loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred\n",
    "            old_prob = y_true * old_prediction\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "            return K.max(r)\n",
    "        \n",
    "        state_input = Input(shape=input_shape)\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(output_shape,))\n",
    "\n",
    "        x = Dense(hidden_layer_neurons, activation='relu')(state_input)\n",
    "        \n",
    "        out_actions = Dense(output_shape, activation='softmax', name='output')(x)\n",
    "\n",
    "        model_train = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model_predict = Model(inputs=[state_input], outputs=[out_actions])\n",
    "        \n",
    "        model_train.compile(Adam(lr), loss=[self.proximal_policy_optimization_loss(advantage, old_prediction)], metrics=[actor_loss])\n",
    "        return model_train, model_predict\n",
    "    \n",
    "    def get_action(self, eval=False):\n",
    "        obs = self.scaler.transform(self.observation.reshape(1, self.nS))\n",
    "        obs = self.observation.reshape(1, self.nS)\n",
    "        p = self.model_predict.predict(obs)\n",
    "        if eval is False:\n",
    "            action = np.random.choice(self.nA, p=p[0]) #np.nan_to_num(p[0])\n",
    "        else:\n",
    "            action = np.argmax(p[0])\n",
    "        action_one_hot = np.zeros(self.nA)\n",
    "        action_one_hot[action] = 1\n",
    "        return action, action_one_hot, p\n",
    "    \n",
    "    def get_entropy(self, preds, epsilon=1e-12):\n",
    "        entropy = np.mean(-np.sum(np.log(preds+epsilon)*preds, axis=1)/np.log(self.nA))\n",
    "        return entropy\n",
    "    \n",
    "    def get_critic_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=1):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
    "#         model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='selu'))\n",
    "        model.add(Dense(output_shape, activation='linear'))\n",
    "        model.compile(Adam(lr), loss=['mse'])\n",
    "        return model\n",
    "    \n",
    "    def get_discounted_rewards(self, r):\n",
    "        # Por si es una lista\n",
    "        r = np.array(r, dtype=float)\n",
    "        \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=10, EPISODES=2000, epochs=1, \n",
    "                                 lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,668\n",
      "Trainable params: 1,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(8)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.model_predict.summary()\n",
    "reinforce_agent.model_predict.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,668\n",
      "Trainable params: 1,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 1) dtype=float32>,\n",
       " <tf.Tensor 'input_3:0' shape=(?, 4) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.model_train.summary()\n",
    "reinforce_agent.model_train.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(values, rewards, gamma=0.999, lmbda=0.95):\n",
    "    #GAE\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] - values[i]\n",
    "        gae = delta + gamma * lmbda * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_targets(rewards, values, gamma=0.999, n_steps = 5):\n",
    "    out = []\n",
    "    ep_len = len(rewards)\n",
    "    gammas = np.power(gamma, range(ep_len+1)) # El +1 es un hack para que no cuelgue cuando le pasamos n_steps = len(rewards)\n",
    "    padded_values = np.vstack([values, np.zeros([n_steps, 1])])\n",
    "    for t in range(ep_len):\n",
    "        # t desde donde comienzo, por ejemplo si t=0 sumo desde 0 a n_steps-1\n",
    "        rewards_left = min([0, ep_len-t-n_steps])\n",
    "        first_term = (gammas[:(n_steps+rewards_left)]*rewards[t:t+n_steps]).sum()\n",
    "        A_t =  first_term - padded_values[t] + gammas[n_steps]*padded_values[t+n_steps]\n",
    "        out.append(A_t)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AC_Advantages(rewards, gamma, values):\n",
    "    return rewards.reshape(-1,1) + gamma*values[1:] - values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=3, EPISODES=2000, epochs=1, \n",
    "                                 lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=3, EPISODES=2000, epochs=1, \n",
    "#                                  lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)\n",
    "# obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(get_advantages(values_, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 51\n",
      "Model on episode 52 improved from -inf to -29.00799929364262. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   4%|▎         | 14/393 [00:00<00:02, 132.58it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmp4maakmd3.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 103\n",
      "Model on episode 104 did not improved -160.15329272956868. Best saved: -29.00799929364262\n",
      "Episode: 155\n",
      "Model on episode 156 did not improved -116.44339108186864. Best saved: -29.00799929364262\n",
      "Episode: 207\n",
      "Model on episode 208 did not improved -92.61671093229256. Best saved: -29.00799929364262\n",
      "Episode: 259\n",
      "Model on episode 260 did not improved -43.16824945854467. Best saved: -29.00799929364262\n",
      "Episode: 311\n",
      "Model on episode 312 did not improved -119.46521444827442. Best saved: -29.00799929364262\n",
      "Episode: 363\n",
      "Model on episode 364 improved from -29.00799929364262 to -12.754226083122163. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|▏         | 14/999 [00:00<00:07, 135.72it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmpsvdxwrdk.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 415\n",
      "Model on episode 416 did not improved -27.25910637738411. Best saved: -12.754226083122163\n",
      "Episode: 467\n",
      "Model on episode 468 did not improved -42.3961014148332. Best saved: -12.754226083122163\n",
      "Episode: 519\n",
      "Model on episode 520 improved from -12.754226083122163 to -4.006265459357406. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|▏         | 14/1001 [00:00<00:07, 133.72it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmph8gt4rjf.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 571\n",
      "Model on episode 572 did not improved -21.419830388021676. Best saved: -4.006265459357406\n",
      "Episode: 623\n",
      "Model on episode 624 did not improved -21.026039913103613. Best saved: -4.006265459357406\n",
      "Episode: 675\n",
      "Model on episode 676 improved from -4.006265459357406 to 36.70298084297842. Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|▏         | 14/1001 [00:00<00:07, 132.18it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file /tmp/tmp29zkqgyq.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 727\n",
      "Model on episode 728 did not improved 10.028054572472021. Best saved: 36.70298084297842\n",
      "Episode: 779\n",
      "Model on episode 780 did not improved -14.403479598293512. Best saved: 36.70298084297842\n",
      "Episode: 831\n",
      "Model on episode 832 did not improved -116.25369456696359. Best saved: 36.70298084297842\n",
      "Episode: 883\n",
      "Model on episode 884 did not improved -59.77599257473154. Best saved: 36.70298084297842\n",
      "Episode: 935\n",
      "Model on episode 936 did not improved 22.921179346324035. Best saved: 36.70298084297842\n",
      "Episode: 987\n",
      "Model on episode 988 did not improved -22.59992648687693. Best saved: 36.70298084297842\n",
      "Episode: 1009"
     ]
    }
   ],
   "source": [
    "critic_lr = 0.001\n",
    "actor_lr =  0.001\n",
    "LOSS_CLIPPING = 0.01 # Recomendado por el Paper\n",
    "ENTROPY_LOSS = 0.0 #5e-4\n",
    "\n",
    "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=10, EPISODES=1000, epochs=10, \n",
    "                                 LOSS_CLIPPING=LOSS_CLIPPING,\n",
    "                                 ENTROPY_LOSS=ENTROPY_LOSS,\n",
    "                                 lr=actor_lr, algorithm='PPO', gif_to_board=True, batch_size=64, gamma=0.99)\n",
    "\n",
    "# reinforce_agent = ReinforceAgent('CartPole-v0', n_experience_episodes=1, EPISODES=2000, epochs=1, \n",
    "#                                  lr=actor_lr, algorithm='PPO', gif_to_board=False, batch_size=64)\n",
    "\n",
    "initial_time = time()\n",
    "running_variance = RunningVariance()\n",
    "critic_model = reinforce_agent.get_critic_model(lr=critic_lr, \n",
    "                                           hidden_layer_neurons=128,\n",
    "                                           input_shape=[reinforce_agent.nS],\n",
    "                                           output_shape=1)\n",
    "\n",
    "###########################################\n",
    "## Entreno V(s) para que no tenga basura ##\n",
    "###########################################\n",
    "# Corro episodios con policy random\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "\n",
    "# Les saco la ultima observación por que no tiene reward\n",
    "observations = []\n",
    "for i in range(reinforce_agent.n_experience_episodes):\n",
    "    observations.append(obs[i][:-1])\n",
    "observations = np.vstack(observations)\n",
    "\n",
    "# Entreno V(s)\n",
    "history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "\n",
    "\n",
    "###########################################\n",
    "## Ciclo de entrenamiento del modelo     ##\n",
    "###########################################\n",
    "\n",
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    # Corro episodio con policy que se irá entrenando\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "    actions = np.vstack(actions) # Pongo todas las acciones de los distintos episodios juntas\n",
    "    # Pongo las predicciones juntas y las guardo como las viejas para pasarselas al modelo\n",
    "    # Las nuevas predicciones será la salida de la red neuronal\n",
    "    old_prediction = np.vstack(preds) \n",
    "    \n",
    "    # Calculo advantages y guardo observaciones sin la última observación\n",
    "    advantage = []\n",
    "    observations = []\n",
    "    for i in range(reinforce_agent.n_experience_episodes):\n",
    "        values = critic_model.predict(obs[i]) \n",
    "#         values_ = np.vstack([rewards[i].reshape(-1,1) + reinforce_agent.gamma*values[1:], 0])\n",
    "        \n",
    "        advantage.append(get_advantages(values, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))\n",
    "#         advantage.append(get_AC_Advantages(rewards[i], reinforce_agent.gamma, values))\n",
    "        observations.append(obs[i][:-1])\n",
    "        \n",
    "    advantage = np.vstack(advantage)\n",
    "    observations = np.vstack(observations)\n",
    "    \n",
    "    # Calculo de varianza\n",
    "    for ad in advantage:\n",
    "        running_variance.add(ad)\n",
    "\n",
    "    # Normalización de advantage\n",
    "    advantage = (advantage-advantage.mean()) / advantage.std()\n",
    "    \n",
    "    # Entrenamiento de Policy\n",
    "    history_loss = reinforce_agent.model_train.fit([observations, advantage, old_prediction], \n",
    "                                                   actions, verbose=0, \n",
    "                                                   epochs=reinforce_agent.epochs, \n",
    "                                                   batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "#     disc_sum_rews = (disc_sum_rews - disc_sum_rews.mean()) / disc_sum_rews.std()\n",
    "    # Entrenamiento de V(s)\n",
    "    history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "    # Logue de resultados\n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history_loss.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      reinforce_agent.get_entropy(old_prediction), \n",
    "                      running_variance.get_variance(), \n",
    "                      history_loss.history['actor_loss'][0], \n",
    "                      time() - initial_time, np.mean(ep_returns[-1]), \n",
    "                      history_critic.history['loss'][0])\n",
    "    \n",
    "reinforce_agent.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
