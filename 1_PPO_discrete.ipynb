{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from REINFORCE_helper import RunningVariance\n",
    "from time import time\n",
    "from REINFORCE_helper import BaseAgent\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent(BaseAgent):\n",
    "    def proximal_policy_optimization_loss(self, advantage, old_prediction):\n",
    "        def loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred\n",
    "            old_prob = y_true * old_prediction\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "            return -K.mean(K.minimum(r * advantage, K.clip(r, min_value=1 - self.LOSS_CLIPPING, max_value=1 + self.LOSS_CLIPPING) * advantage) \n",
    "                           + self.ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "        return loss\n",
    "\n",
    "    def get_policy_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=2):\n",
    "        ## Defino m√©trica - loss sin el retorno multiplicando\n",
    "        def actor_loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred\n",
    "            old_prob = y_true * old_prediction\n",
    "            r = prob/(old_prob + 1e-10)\n",
    "            return K.max(r)\n",
    "        \n",
    "        state_input = Input(shape=input_shape)\n",
    "        advantage = Input(shape=(1,))\n",
    "        old_prediction = Input(shape=(output_shape,))\n",
    "\n",
    "        x = Dense(hidden_layer_neurons, activation='relu')(state_input)\n",
    "        \n",
    "        out_actions = Dense(output_shape, activation='softmax', name='output')(x)\n",
    "\n",
    "        model_train = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "        model_predict = Model(inputs=[state_input], outputs=[out_actions])\n",
    "        \n",
    "        model_train.compile(Adam(lr), loss=[self.proximal_policy_optimization_loss(advantage, old_prediction)], metrics=[actor_loss])\n",
    "        return model_train, model_predict\n",
    "    \n",
    "    def get_action(self, eval=False):\n",
    "        obs = self.scaler.transform(self.observation.reshape(1, self.nS))\n",
    "        obs = self.observation.reshape(1, self.nS)\n",
    "        p = self.model_predict.predict(obs)\n",
    "        if eval is False:\n",
    "            action = np.random.choice(self.nA, p=p[0]) #np.nan_to_num(p[0])\n",
    "        else:\n",
    "            action = np.argmax(p[0])\n",
    "        action_one_hot = np.zeros(self.nA)\n",
    "        action_one_hot[action] = 1\n",
    "        return action, action_one_hot, p\n",
    "    \n",
    "    def get_entropy(self, preds, epsilon=1e-12):\n",
    "        entropy = np.mean(-np.sum(np.log(preds+epsilon)*preds, axis=1)/np.log(self.nA))\n",
    "        return entropy\n",
    "    \n",
    "    def get_critic_model(self, lr=0.001, hidden_layer_neurons = 128, input_shape=[4], output_shape=1):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='relu'))\n",
    "#         model.add(Dense(hidden_layer_neurons, input_shape=input_shape, activation='selu'))\n",
    "        model.add(Dense(output_shape, activation='linear'))\n",
    "        model.compile(Adam(lr), loss=['mse'])\n",
    "        return model\n",
    "    \n",
    "    def get_discounted_rewards(self, r):\n",
    "        # Por si es una lista\n",
    "        r = np.array(r, dtype=float)\n",
    "        \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "        discounted_r = np.zeros_like(r)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, r.size)):\n",
    "            running_add = running_add * self.gamma + r[t]\n",
    "            discounted_r[t] = running_add\n",
    "        return discounted_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/usuario/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=10, EPISODES=2000, epochs=1, \n",
    "                                 lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,668\n",
      "Trainable params: 1,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(8)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.model_predict.summary()\n",
    "reinforce_agent.model_predict.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,668\n",
      "Trainable params: 1,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 8) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 1) dtype=float32>,\n",
       " <tf.Tensor 'input_3:0' shape=(?, 4) dtype=float32>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce_agent.model_train.summary()\n",
    "reinforce_agent.model_train.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(values, rewards, gamma=0.999, lmbda=0.95):\n",
    "    #GAE\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] - values[i]\n",
    "        gae = delta + gamma * lmbda * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_targets(rewards, values, gamma=0.999, n_steps = 5):\n",
    "    out = []\n",
    "    ep_len = len(rewards)\n",
    "    gammas = np.power(gamma, range(ep_len+1)) # El +1 es un hack para que no cuelgue cuando le pasamos n_steps = len(rewards)\n",
    "    padded_values = np.vstack([values, np.zeros([n_steps, 1])])\n",
    "    for t in range(ep_len):\n",
    "        # t desde donde comienzo, por ejemplo si t=0 sumo desde 0 a n_steps-1\n",
    "        rewards_left = min([0, ep_len-t-n_steps])\n",
    "        first_term = (gammas[:(n_steps+rewards_left)]*rewards[t:t+n_steps]).sum()\n",
    "        A_t =  first_term - padded_values[t] + gammas[n_steps]*padded_values[t+n_steps]\n",
    "        out.append(A_t)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AC_Advantages(rewards, gamma, values):\n",
    "    return rewards.reshape(-1,1) + gamma*values[1:] - values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=3, EPISODES=2000, epochs=1, \n",
    "                                 lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=3, EPISODES=2000, epochs=1, \n",
    "#                                  lr=0.001, algorithm='REINFORCE_V_s', gif_to_board=True, batch_size=32)\n",
    "# obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.plot(get_advantages(values_, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correr en linea de comando: tensorboard --logdir logs/\n",
      "Episode: 51\n",
      "Model on episode 52 improved from -inf to -122.7101183944498. Saved!\n",
      "Episode: 103\n",
      "Model on episode 104 improved from -122.7101183944498 to 13.252529696117886. Saved!\n",
      "Episode: 155\n",
      "Model on episode 156 did not improved -21.29135259240424. Best saved: 13.252529696117886\n",
      "Episode: 207\n",
      "Model on episode 208 did not improved -19.773217154317233. Best saved: 13.252529696117886\n",
      "Episode: 259\n",
      "Model on episode 260 did not improved -19.81648339495491. Best saved: 13.252529696117886\n",
      "Episode: 311\n",
      "Model on episode 312 improved from 13.252529696117886 to 22.46259275266471. Saved!\n",
      "Episode: 363\n",
      "Model on episode 364 improved from 22.46259275266471 to 24.73041717698794. Saved!\n",
      "Episode: 415\n",
      "Model on episode 416 did not improved -5.102755587407028. Best saved: 24.73041717698794\n",
      "Episode: 467\n",
      "Model on episode 468 did not improved 19.191241586085194. Best saved: 24.73041717698794\n",
      "Episode: 519\n",
      "Model on episode 520 improved from 24.73041717698794 to 26.770816901147942. Saved!\n",
      "Episode: 541"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cbb5fa2dbce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n\u001b[1;32m     81\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreinforce_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                       batch_size=reinforce_agent.batch_size)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Logue de resultados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GPUV2/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "critic_lr = 0.001\n",
    "actor_lr =  0.001\n",
    "LOSS_CLIPPING = 0.2 # Recomendado por el Paper\n",
    "ENTROPY_LOSS = 1 #5e-4\n",
    "\n",
    "reinforce_agent = ReinforceAgent('LunarLander-v2', n_experience_episodes=10, EPISODES=1000, epochs=10, \n",
    "                                 LOSS_CLIPPING=LOSS_CLIPPING,\n",
    "                                 ENTROPY_LOSS=ENTROPY_LOSS,\n",
    "                                 lr=actor_lr, algorithm='PPO', gif_to_board=False, batch_size=64, gamma=0.99)\n",
    "\n",
    "# reinforce_agent = ReinforceAgent('CartPole-v0', n_experience_episodes=1, EPISODES=2000, epochs=1, \n",
    "#                                  lr=actor_lr, algorithm='PPO', gif_to_board=False, batch_size=64)\n",
    "\n",
    "initial_time = time()\n",
    "running_variance = RunningVariance()\n",
    "critic_model = reinforce_agent.get_critic_model(lr=critic_lr, \n",
    "                                           hidden_layer_neurons=128,\n",
    "                                           input_shape=[reinforce_agent.nS],\n",
    "                                           output_shape=1)\n",
    "\n",
    "###########################################\n",
    "## Entreno V(s) para que no tenga basura ##\n",
    "###########################################\n",
    "# Corro episodios con policy random\n",
    "obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "\n",
    "# Les saco la ultima observaci√≥n por que no tiene reward\n",
    "observations = []\n",
    "for i in range(reinforce_agent.n_experience_episodes):\n",
    "    observations.append(obs[i][:-1])\n",
    "observations = np.vstack(observations)\n",
    "\n",
    "# Entreno V(s)\n",
    "history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "\n",
    "\n",
    "###########################################\n",
    "## Ciclo de entrenamiento del modelo     ##\n",
    "###########################################\n",
    "\n",
    "while reinforce_agent.episode < reinforce_agent.EPISODES:\n",
    "    # Corro episodio con policy que se ir√° entrenando\n",
    "    obs, actions, preds, disc_sum_rews, rewards, ep_returns, ep_len, time_steps = reinforce_agent.get_experience_episodes(return_ts=True)\n",
    "    actions = np.vstack(actions) # Pongo todas las acciones de los distintos episodios juntas\n",
    "    # Pongo las predicciones juntas y las guardo como las viejas para pasarselas al modelo\n",
    "    # Las nuevas predicciones ser√° la salida de la red neuronal\n",
    "    old_prediction = np.vstack(preds) \n",
    "    \n",
    "    # Calculo advantages y guardo observaciones sin la √∫ltima observaci√≥n\n",
    "    advantage = []\n",
    "    observations = []\n",
    "    for i in range(reinforce_agent.n_experience_episodes):\n",
    "        values = critic_model.predict(obs[i]) \n",
    "#         values_ = np.vstack([rewards[i].reshape(-1,1) + reinforce_agent.gamma*values[1:], 0])\n",
    "        \n",
    "        advantage.append(get_advantages(values, rewards[i], gamma=reinforce_agent.gamma, lmbda=0.1))\n",
    "#         advantage.append(get_AC_Advantages(rewards[i], reinforce_agent.gamma, values))\n",
    "        observations.append(obs[i][:-1])\n",
    "        \n",
    "    advantage = np.vstack(advantage)\n",
    "    observations = np.vstack(observations)\n",
    "    \n",
    "    # Calculo de varianza\n",
    "    for ad in advantage:\n",
    "        running_variance.add(ad)\n",
    "\n",
    "    # Normalizaci√≥n de advantage\n",
    "    advantage = (advantage-advantage.mean()) / advantage.std()\n",
    "    \n",
    "    # Entrenamiento de Policy\n",
    "    history_loss = reinforce_agent.model_train.fit([observations, advantage, old_prediction], \n",
    "                                                   actions, verbose=0, \n",
    "                                                   epochs=reinforce_agent.epochs, \n",
    "                                                   batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "#     disc_sum_rews = (disc_sum_rews - disc_sum_rews.mean()) / disc_sum_rews.std()\n",
    "    # Entrenamiento de V(s)\n",
    "    history_critic = critic_model.fit(observations, np.vstack(disc_sum_rews), verbose=0, \n",
    "                                      epochs=reinforce_agent.epochs,\n",
    "                                      batch_size=reinforce_agent.batch_size)\n",
    "    \n",
    "    # Logue de resultados\n",
    "    reinforce_agent.log_data(reinforce_agent.episode, \n",
    "                      history_loss.history['loss'][0], \n",
    "                      np.mean(ep_len), \n",
    "                      reinforce_agent.get_entropy(old_prediction), \n",
    "                      running_variance.get_variance(), \n",
    "                      history_loss.history['actor_loss'][0], \n",
    "                      time() - initial_time, np.mean(ep_returns[-1]), \n",
    "                      history_critic.history['loss'][0])\n",
    "    \n",
    "reinforce_agent.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
